梯度下降 (Gradient descent) 法在在 AI 的領域中是很常使用的算法，核心概念就是直接解他的差分方程並得到最小值，當然也可以用 QR 分解來求得，且能夠得到較精確的最小值，但是就需要花較多的時間，梯度下降則是捨去精度來換時間。其中又有共軛(conjugate)梯度法、隨機(stochastic)梯度法與調步(Adaptive)梯度法。接著從最基本的再逐一介紹。

## 1. 數學理論
在最佳化問題中就是利用微分去找極值，在此我們要找的是極(最)小值，在數學上的條件為 $\nabla f(x) = 0$ 且 $\nabla^2 f(x) > 0$。而二階微分在數值求解通常會算很久，所以多數只用一階微分來判斷，所以有時候並不知道是否存在一個非邊界點的極(最)小值。例如 $x^3$ 就沒有除了邊界以外的極值， $x^2 - y^2$ 則是有鞍點(saddle point)。在此我們就介紹一些數值方法來找極(最)小值。

## 2. 梯度下降法
對於一個有 Global/Local minimum 的函數而言，我們可以由一階微分來做尋找，並直接帶入原函數就可以比較出結果。例如\
f(x) = 2x<sup>2</sup> + 4x + 3, f'(x) = 4x + 4 --> f<sub>min</sub>(x*) = (-1, 1)\
而電腦無法算微分，所以轉為插分方程可得\
$f'(x) = \dfrac{df(x)}{dx} \sim \dfrac{ \Delta f(x)}{ \Delta x} = \dfrac{df(x_2) - df(x_1)}{x_2 - x_1}$\
所以就直接將 x 帶入去找到 x*\
$x_{n+1} = x_n - \lambda f'(x_n)$\
其中 $\lambda$ 就是每一步要跨多大步，太大步有可能走不到最小值，但太小步又會跑太久，所以就有以上最後提到的方法。而實際問題是很多維的，所以只要分個別維度去做即可。

## 2. 共軛梯度法
